import json
import time
import concurrent.futures
import psutil
import requests
import numpy as np
import threading
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd
import json

# å…¨å±€è®¾ç½®
plt.rcParams['font.family'] = 'DejaVu Sans'  # å…¼å®¹æ€§æ›´å¥½çš„å­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·

# é…ç½®å‚æ•°
URL = "http://127.0.0.1:5001/ask_videx"  # æ›¿æ¢ä¸ºå®é™…çš„æ¥å£åœ°å€
CONCURRENCY_LEVELS = [10, 50, 100, 150, 200]
REQUESTS_PER_LEVEL = 2000  # æ¯ä¸ªå¹¶å‘çº§åˆ«çš„è¯·æ±‚æ€»æ•°
SAMPLE_INTERVAL = 0.05  # ç³»ç»Ÿç›‘æ§é‡‡æ ·é—´éš”(ç§’)


class SystemMetrics:
    """ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.cpu = []
        self.mem = []
        self.cpu_min = float('inf')
        self.cpu_max = 0
        self.cpu_avg = 0
        self.mem_min = float('inf')
        self.mem_max = 0
        self.mem_avg = 0
        self.start_time = time.time()


class PerformanceMonitor(threading.Thread):
    """åå°æ€§èƒ½ç›‘æ§çº¿ç¨‹"""

    def __init__(self, interval):
        super().__init__()
        self.interval = interval
        self.metrics = SystemMetrics()
        self.running = True
        self.daemon = True

    def run(self):
        while self.running:
            cpu_percent = psutil.cpu_percent(interval=self.interval)
            mem_percent = psutil.virtual_memory().percent

            self.metrics.cpu.append(cpu_percent)
            self.metrics.mem.append(mem_percent)

            # æ›´æ–°æœ€å°å€¼
            if cpu_percent < self.metrics.cpu_min:
                self.metrics.cpu_min = cpu_percent
            if mem_percent < self.metrics.mem_min:
                self.metrics.mem_min = mem_percent

            # æ›´æ–°æœ€å¤§å€¼
            if cpu_percent > self.metrics.cpu_max:
                self.metrics.cpu_max = cpu_percent
            if mem_percent > self.metrics.mem_max:
                self.metrics.mem_max = mem_percent

            if not self.running:
                break

    def stop(self):
        self.running = False
        # è®¡ç®—å¹³å‡å€¼
        if self.metrics.cpu:
            self.metrics.cpu_avg = sum(self.metrics.cpu) / len(self.metrics.cpu)
        if self.metrics.mem:
            self.metrics.mem_avg = sum(self.metrics.mem) / len(self.metrics.mem)
        self.metrics.end_time = time.time()
        return self.metrics


def send_request(url, request_data):
    """å‘é€å•ä¸ªè¯·æ±‚å¹¶è®°å½•æŒ‡æ ‡"""
    start_time = time.time()
    try:
        headers = {'Content-Type': 'application/json'}
        response = requests.post(url, headers=headers, json=request_data, timeout=10)
        response.raise_for_status()
        return True, response.status_code, time.time() - start_time
    except requests.Timeout:
        return False, 504, time.time() - start_time
    except requests.RequestException as e:
        status_code = e.response.status_code if hasattr(e, 'response') and e.response else 500
        return False, status_code, time.time() - start_time


def run_load_test(concurrency, total_requests):
    """æ‰§è¡ŒæŒ‡å®šå¹¶å‘çº§åˆ«çš„è´Ÿè½½æµ‹è¯•"""
    print(f"\nğŸš€ Starting load test with concurrency: {concurrency}, requests: {total_requests}")

    # å¯åŠ¨æ€§èƒ½ç›‘æ§
    monitor = PerformanceMonitor(SAMPLE_INTERVAL)
    monitor.start()

    # åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®ç»“æ„
    response_times = []
    successful_count = 0
    error_count = 0
    status_codes = {}

    start_time = time.time()

    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        futures = []
        for _ in range(total_requests):
            for request_data in json_requests:
                future = executor.submit(send_request, URL, request_data)
                futures.append(future)

        # æ”¶é›†æ¯ä¸ªè¯·æ±‚çš„ç»“æœ
        for future in concurrent.futures.as_completed(futures):
            success, status_code, response_time = future.result()

            response_times.append(response_time)
            status_codes[status_code] = status_codes.get(status_code, 0) + 1

            if success:
                successful_count += 1
            else:
                error_count += 1

    end_time = time.time()
    total_duration = end_time - start_time

    # åœæ­¢æ€§èƒ½ç›‘æ§å¹¶è·å–ç»“æœ
    metrics = monitor.stop()

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    min_time = min(response_times) if response_times else 0
    max_time = max(response_times) if response_times else 0
    avg_time = sum(response_times) / len(response_times) if response_times else 0
    qps = total_requests / total_duration if total_duration > 0 else 0
    error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0

    # åˆ†ä½æ•°è®¡ç®—
    quantiles = {'p50': 0, 'p90': 0, 'p95': 0, 'p99': 0}
    if response_times:
        sorted_times = sorted(response_times)
        quantiles['p50'] = np.percentile(sorted_times, 50)
        quantiles['p90'] = np.percentile(sorted_times, 90)
        quantiles['p95'] = np.percentile(sorted_times, 95)
        quantiles['p99'] = np.percentile(sorted_times, 99)

    # è¿”å›ç»“æœ
    return {
        'concurrency': concurrency,
        'total_requests': total_requests,
        'successful_count': successful_count,
        'error_count': error_count,
        'error_rate': error_rate,
        'start_time': start_time,
        'end_time': end_time,
        'total_duration': total_duration,
        'min_response_time': min_time,
        'max_response_time': max_time,
        'avg_response_time': avg_time,
        'qps': qps,
        'response_time_quantiles': quantiles,
        'cpu_min': metrics.cpu_min,
        'cpu_max': metrics.cpu_max,
        'cpu_avg': metrics.cpu_avg,
        'mem_min': metrics.mem_min,
        'mem_max': metrics.mem_max,
        'mem_avg': metrics.mem_avg,
        'status_codes': status_codes
    }


def plot_comparison_results(results):
    """ç»˜åˆ¶ä¸åŒå¹¶å‘çº§åˆ«çš„å¯¹æ¯”å›¾è¡¨"""
    plt.figure(figsize=(15, 18))

    # å‡†å¤‡æ•°æ®
    concurrency = [r['concurrency'] for r in results]
    qps = [r['qps'] for r in results]
    avg_response = [r['avg_response_time'] * 1000 for r in results]  # è½¬æ¢ä¸ºæ¯«ç§’
    cpu_avg = [r['cpu_avg'] for r in results]
    mem_avg = [r['mem_avg'] for r in results]
    error_rates = [r['error_rate'] for r in results]

    # 1. QPSå’Œé”™è¯¯ç‡å¯¹æ¯”
    plt.subplot(4, 1, 1)
    fig1 = plt.gca()
    fig1.set_title('QPS and Error Rate by Concurrency Level', fontsize=14)

    # QPS - å·¦è½´
    fig1.set_xlabel('Concurrent Threads')
    fig1.set_ylabel('QPS (Requests/Sec)', color='blue')
    fig1.plot(concurrency, qps, 'o-', markersize=8, linewidth=2, color='blue')
    fig1.tick_params(axis='y', labelcolor='blue')
    fig1.grid(True, linestyle='--', alpha=0.7)

    # é”™è¯¯ç‡ - å³è½´
    fig2 = fig1.twinx()
    fig2.set_ylabel('Error Rate (%)', color='red')
    fig2.plot(concurrency, error_rates, 's-', markersize=8, linewidth=2, color='red')
    fig2.tick_params(axis='y', labelcolor='red')

    # æ·»åŠ æ•°æ®æ ‡ç­¾
    for i, v in enumerate(qps):
        fig1.text(concurrency[i], v, f'{v:.1f}', ha='center', va='bottom', fontsize=9)

    for i, v in enumerate(error_rates):
        fig2.text(concurrency[i], v + 0.2, f'{v:.1f}%', ha='center', va='bottom', fontsize=9, color='red')

    # 2. å“åº”æ—¶é—´å¯¹æ¯”
    plt.subplot(4, 1, 2)
    avg_response_patch = mpatches.Patch(color='blue', label='Avg Response Time')
    p95_patch = mpatches.Patch(color='green', label='P95 Response Time')
    p99_patch = mpatches.Patch(color='red', label='P99 Response Time')
    plt.legend(handles=[avg_response_patch, p95_patch, p99_patch], loc='upper left')

    # å¹³å‡å“åº”æ—¶é—´
    plt.plot(concurrency, avg_response, 'o-', markersize=8, linewidth=2, color='blue')

    # P95å“åº”æ—¶é—´
    p95_responses = [r['response_time_quantiles']['p95'] * 1000 for r in results]
    plt.plot(concurrency, p95_responses, 's-', markersize=8, linewidth=2, color='green')

    # P99å“åº”æ—¶é—´
    p99_responses = [r['response_time_quantiles']['p99'] * 1000 for r in results]
    plt.plot(concurrency, p99_responses, 'D-', markersize=8, linewidth=2, color='red')

    plt.title('Response Time Metrics by Concurrency Level', fontsize=14)
    plt.xlabel('Concurrent Threads')
    plt.ylabel('Response Time (ms)')
    plt.grid(True, linestyle='--', alpha=0.7)

    # æ·»åŠ æ•°æ®æ ‡ç­¾
    for i, v in enumerate(avg_response):
        plt.text(concurrency[i], v, f'{v:.1f}ms', ha='center', va='bottom', fontsize=8)

    for i, v in enumerate(p95_responses):
        plt.text(concurrency[i], v + 10, f'P95:{v:.1f}ms', ha='center', va='bottom', fontsize=8, color='green')

    for i, v in enumerate(p99_responses):
        plt.text(concurrency[i], v + 20, f'P99:{v:.1f}ms', ha='center', va='bottom', fontsize=8, color='red')

    # 3. èµ„æºä½¿ç”¨ç‡å¯¹æ¯”
    plt.subplot(4, 1, 3)
    fig3 = plt.gca()
    fig3.set_title('System Resource Usage by Concurrency Level', fontsize=14)

    # CPUä½¿ç”¨ç‡ - å·¦è½´
    fig3.set_xlabel('Concurrent Threads')
    fig3.set_ylabel('CPU Usage (%)', color='purple')
    fig3.plot(concurrency, cpu_avg, 'o-', markersize=8, linewidth=2, color='purple')
    fig3.tick_params(axis='y', labelcolor='purple')
    fig3.grid(True, linestyle='--', alpha=0.7)

    # å†…å­˜ä½¿ç”¨ç‡ - å³è½´
    fig4 = fig3.twinx()
    fig4.set_ylabel('Memory Usage (%)', color='orange')
    fig4.plot(concurrency, mem_avg, 's-', markersize=8, linewidth=2, color='orange')
    fig4.tick_params(axis='y', labelcolor='orange')

    # æ·»åŠ æ•°æ®æ ‡ç­¾
    for i, v in enumerate(cpu_avg):
        fig3.text(concurrency[i], v, f'{v:.1f}%', ha='center', va='bottom', fontsize=9, color='purple')

    for i, v in enumerate(mem_avg):
        fig4.text(concurrency[i], v, f'{v:.1f}%', ha='center', va='bottom', fontsize=9, color='orange')

    # 4. çŠ¶æ€ç åˆ†å¸ƒ
    plt.subplot(4, 1, 4)

    # åˆ›å»ºçŠ¶æ€ç å †å å›¾
    status_categories = {
        200: '#4CAF50',  # æˆåŠŸ - ç»¿è‰²
        201: '#8BC34A',  # åˆ›å»ºæˆåŠŸ - æµ…ç»¿
        204: '#CDDC39',  # æ— å†…å®¹ - é»„ç»¿
        400: '#FFC107',  # é”™è¯¯è¯·æ±‚ - é»„è‰²
        401: '#FF9800',  # æœªæˆæƒ - æ©™è‰²
        403: '#FF5722',  # ç¦æ­¢è®¿é—® - æ·±æ©™
        404: '#795548',  # æœªæ‰¾åˆ° - æ£•è‰²
        500: '#F44336',  # æœåŠ¡å™¨é”™è¯¯ - çº¢è‰²
        502: '#E91E63',  # ç½‘å…³é”™è¯¯ - ç²‰è‰²
        503: '#9C27B0',  # æœåŠ¡ä¸å¯ç”¨ - ç´«è‰²
        504: '#673AB7'  # ç½‘å…³è¶…æ—¶ - æ·±ç´«
    }

    # è·å–æ‰€æœ‰çŠ¶æ€ç å¹¶æŒ‰é¢‘ç‡æ’åº
    all_codes = set()
    for result in results:
        for code in result['status_codes']:
            all_codes.add(code)
    sorted_codes = sorted(all_codes)

    # ä¸ºæ¯ä¸ªå¹¶å‘çº§åˆ«åˆ›å»ºçŠ¶æ€ç è®¡æ•°åˆ—è¡¨
    bar_width = 70  # æ¡å½¢å®½åº¦
    x_positions = np.arange(len(concurrency)) * bar_width * 1.5

    bottoms = np.zeros(len(concurrency))

    for code in sorted_codes:
        counts = []
        for result in results:
            counts.append(result['status_codes'].get(code, 0))

        color = status_categories.get(code, '#CCCCCC')  # é»˜è®¤ç°è‰²
        label = f'{code}'

        plt.bar(x_positions, counts, bar_width, bottom=bottoms, color=color, label=label)
        bottoms += counts

    plt.title('Status Code Distribution by Concurrency Level', fontsize=14)
    plt.xlabel('Concurrent Threads')
    plt.ylabel('Number of Responses')
    plt.xticks(x_positions, concurrency)
    plt.grid(True, linestyle='--', alpha=0.7, axis='y')

    # æ·»åŠ å›¾ä¾‹
    plt.legend(title='Status Codes', bbox_to_anchor=(1.05, 1), loc='upper left')

    # æ·»åŠ æ€»æ•°æ ‡ç­¾
    for i, total in enumerate(bottoms):
        plt.text(x_positions[i], total + 5, f'Total: {total:.0f}',
                 ha='center', fontsize=9)

    plt.tight_layout()

    # ä¿å­˜ç»“æœ
    plot_file = "result/concurrency_comparison.png"
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    print(f"\nâœ… Comparison plot saved to: {plot_file}")

    return plot_file


def save_results_to_excel(results, filename="æ‰§è¡Œç»“æœè®°å½•/concurrency_test_results.xlsx"):
    """å°†æµ‹è¯•ç»“æœä¿å­˜åˆ°Excelæ–‡ä»¶"""
    data = []

    for r in results:
        # æ ¼å¼åŒ–çŠ¶æ€ç åˆ†å¸ƒ
        status_distribution = json.dumps(r['status_codes'])

        # æ·»åŠ ç»Ÿè®¡
        success_rate = (r['successful_count'] / r['total_requests'] * 100) if r['total_requests'] > 0 else 0
        failure_rate = 100 - success_rate

        data.append({
            'Concurrency Level': r['concurrency'],
            'Total Requests': r['total_requests'],
            'Successful Requests': r['successful_count'],
            'Failed Requests': r['error_count'],
            'Success Rate (%)': f"{success_rate:.2f}",
            'Error Rate (%)': f"{r['error_rate']:.2f}",
            'Test Duration (s)': f"{r['total_duration']:.2f}",
            'QPS': f"{r['qps']:.2f}",
            'Min Response Time (ms)': f"{r['min_response_time'] * 1000:.2f}",
            'Avg Response Time (ms)': f"{r['avg_response_time'] * 1000:.2f}",
            'Max Response Time (ms)': f"{r['max_response_time'] * 1000:.2f}",
            'P50 (ms)': f"{r['response_time_quantiles']['p50'] * 1000:.2f}",
            'P90 (ms)': f"{r['response_time_quantiles']['p90'] * 1000:.2f}",
            'P95 (ms)': f"{r['response_time_quantiles']['p95'] * 1000:.2f}",
            'P99 (ms)': f"{r['response_time_quantiles']['p99'] * 1000:.2f}",
            'Min CPU (%)': f"{r['cpu_min']:.1f}",
            'Avg CPU (%)': f"{r['cpu_avg']:.1f}",
            'Max CPU (%)': f"{r['cpu_max']:.1f}",
            'Min Memory (%)': f"{r['mem_min']:.1f}",
            'Avg Memory (%)': f"{r['mem_avg']:.1f}",
            'Max Memory (%)': f"{r['mem_max']:.1f}",
            'Status Code Distribution': status_distribution
        })

    df = pd.DataFrame(data)
    df.to_excel(filename, index=False)
    print(f"\nğŸ“Š Results saved to Excel file: {filename}")
    return filename


def main():
    print(f"Starting performance comparison test")

    all_results = []

    # è¿è¡Œæ‰€æœ‰å¹¶å‘çº§åˆ«æµ‹è¯•
    for concurrency in CONCURRENCY_LEVELS:
        result = run_load_test(concurrency, REQUESTS_PER_LEVEL)
        all_results.append(result)

    # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
    plot_file = plot_comparison_results(all_results)

    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°Excel
    # excel_file = save_results_to_excel(all_results)

    print("\n==================================")
    print("ğŸš€ Performance comparison complete!")
    print("==================================")


if __name__ == "__main__":
    # å®šä¹‰æµ‹è¯•è¯·æ±‚æ•°æ®ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    json_requests = [
        {
            "item_type": "videx_root",
            "properties": {
                "dbname": "videx_1",
                "function": "virtual double ha_videx::get_memory_buffer_size()",
                "table_name": "orders",
                "target_storage_engine": "VIDEX"
            },
            "data": []
        },
        {
            "item_type": "videx_root",
            "properties": {
                "dbname": "videx_1",
                "function": "virtual double ha_videx::scan_time()",
                "table_name": "orders",
                "target_storage_engine": "VIDEX"
            },
            "data": []
        },
        {
            "item_type": "videx_request",
            "properties": {
                "dbname": "videx_1",
                "function": "virtual ha_rows ha_innobase::records_in_range(uint, key_range*, key_range*)",
                "table_name": "orders",
                "target_storage_engine": "VIDEX"
            },
            "data": [
                {
                    "item_type": "min_key",
                    "properties": {
                        "index_name": "ORDERS_FK1",
                        "length": "4",
                        "operator": ">"
                    },
                    "data": [
                        {
                            "item_type": "column_and_bound",
                            "properties": {
                                "column": "O_CUSTKEY",
                                "value": "3"
                            },
                            "data": []
                        }
                    ]
                },
                {
                    "item_type": "max_key",
                    "properties": {
                        "index_name": "ORDERS_FK1",
                        "length": "4",
                        "operator": ">"
                    },
                    "data": [
                        {
                            "item_type": "column_and_bound",
                            "properties": {
                                "column": "O_CUSTKEY",
                                "value": "3"
                            },
                            "data": []
                        }
                    ]
                }
            ]
        },

        {
            "item_type": "videx_request",
            "properties": {
                "dbname": "videx_1",
                "function": "virtual int ha_videx::info_low(uint, bool)",
                "table_name": "orders",
                "target_storage_engine": "VIDEX",
                # "videx_options": "{\"task_id\": \"127_0_0_1_13308@@@demo_tpch\", \"use_gt\": true}"
            },
            "data": [
                {
                    "item_type": "key",
                    "properties": {
                        "key_length": "4",
                        "name": "PRIMARY"
                    },
                    "data": [
                        {
                            "item_type": "field",
                            "properties": {
                                "name": "O_ORDERKEY",
                                "store_length": "4"
                            },
                            "data": []
                        }
                    ]
                },
                {
                    "item_type": "key",
                    "properties": {
                        "key_length": "4",
                        "name": "ORDERS_FK1"
                    },
                    "data": [
                        {
                            "item_type": "field",
                            "properties": {
                                "name": "O_CUSTKEY",
                                "store_length": "4"
                            },
                            "data": []
                        },
                        {
                            "item_type": "field",
                            "properties": {
                                "name": "O_ORDERKEY",
                                "store_length": "4"
                            },
                            "data": []
                        }
                    ]
                }
            ]
        }
    ]

    main()
